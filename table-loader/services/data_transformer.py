# table-loader/services/data_transformer.py
import logging
from datetime import date, datetime
from typing import Any, Dict, List, Optional, Set, Union

import pandas as pd
from core.database import db_manager

logger = logging.getLogger(__name__)


class DataTransformer:
    """Transforms fragment data for database loading"""

    # System columns that should NEVER be loaded (auto-generated by DB)
    SYSTEM_COLUMNS: Set[str] = {
        "Id",
        "created_at",
        "updated_at",
    }

    # Table-specific default exclusions (fields that should ALWAYS be excluded)
    TABLE_DEFAULT_EXCLUSIONS = {
        "local_subject_ids": {
            "action",  # GSID resolution metadata, not a DB column
        },
    }

    def __init__(self, table_name: str, exclude_fields: Set[str] = None):
        """
        Initialize transformer

        Args:
            table_name: Target table name
            exclude_fields: Fields to exclude from loading (from validation report)
        """
        self.table_name = table_name

        # Start with system columns
        self.exclude_fields = self.SYSTEM_COLUMNS.copy()

        # Add table-specific defaults
        if table_name in self.TABLE_DEFAULT_EXCLUSIONS:
            self.exclude_fields.update(self.TABLE_DEFAULT_EXCLUSIONS[table_name])

        # Add fields from validation report
        if exclude_fields:
            self.exclude_fields.update(exclude_fields)

        # Fetch actual schema to protect against loading non-existent fields
        # and to inform type conversion
        try:
            self.schema = db_manager.get_table_schema(table_name)
            self.schema_columns = set(self.schema.keys())
            logger.info(
                f"Fetched {len(self.schema_columns)} schema columns for table '{table_name}'"
            )
        except Exception as e:
            logger.error(
                f"Could not fetch schema for table '{table_name}': {e}. "
                "Proceeding without schema-aware transformations."
            )
            self.schema = {}
            self.schema_columns = None

        logger.info(f"Excluding fields: {sorted(self.exclude_fields)}")

    def transform_records(
        self, data: Union[pd.DataFrame, Dict]
    ) -> List[Dict[str, Any]]:
        """
        Transform DataFrame or dict to list of records for database insertion

        Args:
            data: DataFrame or dict with 'records' key

        Returns:
            List of transformed records ready for database insertion
        """
        # Convert to DataFrame if needed
        if isinstance(data, dict):
            if "records" in data:
                df = pd.DataFrame(data["records"])
            else:
                df = pd.DataFrame([data])
        else:
            df = data

        if df.empty:
            logger.warning(f"No records to transform for {self.table_name}")
            return []

        # Get all columns from DataFrame
        all_columns = set(df.columns)

        # Determine which fields to keep
        fields_to_keep = all_columns - self.exclude_fields

        # If we have the schema, only keep fields that exist in the database table
        if self.schema_columns:
            original_fields = fields_to_keep.copy()
            fields_to_keep = fields_to_keep.intersection(self.schema_columns)
            removed_fields = original_fields - fields_to_keep
            if removed_fields:
                logger.warning(
                    f"Ignoring columns not found in '{self.table_name}' schema: {sorted(removed_fields)}"
                )

        logger.info(f"Fields to load for {self.table_name}: {sorted(fields_to_keep)}")

        # Convert DataFrame to list of dicts
        records = df.to_dict("records")

        # Transform each record
        transformed_records = []
        for record in records:
            # Type conversions and null handling
            transformed = self._transform_record(record)

            # Additional validation: skip records with invalid global_subject_id
            if "global_subject_id" in transformed:
                gsid = transformed.get("global_subject_id")

                # Check for various forms of invalid values
                if (
                    gsid is None
                    or gsid == ""
                    or pd.isna(gsid)
                    or str(gsid).lower() == "nan"
                ):
                    logger.warning(
                        f"Skipping record with invalid global_subject_id: {gsid}"
                    )
                    continue

            # Filter to only include fields we want to load
            filtered_record = {
                k: v for k, v in transformed.items() if k in fields_to_keep
            }
            transformed_records.append(filtered_record)

        logger.info(
            f"Transformed {len(transformed_records)} records for {self.table_name}"
        )

        return transformed_records

    def _transform_record(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """Transform a single record with type conversions"""
        transformed = {}
        for key, value in record.items():
            db_type = self.schema.get(key)
            transformed[key] = self._convert_value(value, db_type)
        return transformed

    def _convert_value(self, value: Any, db_type: Optional[str]) -> Any:
        """
        Convert value to appropriate Python type based on DB schema.
        """
        if value is None or pd.isna(value) or str(value).strip().upper() in ("NULL", "NA", "N/A", ""):
            return None

        # Already correct type
        if isinstance(value, (int, float, bool, date, datetime)):
            return value

        # Convert to string for consistent processing
        str_value = str(value).strip()

        # Perform conversion based on target database type
        if db_type:
            if 'int' in db_type:
                try:
                    return int(float(str_value))
                except (ValueError, TypeError):
                    logger.warning(f"Could not convert '{value}' to int for column with type {db_type}")
                    return None
            elif db_type in ('decimal', 'numeric', 'real', 'double precision'):
                try:
                    return float(str_value)
                except (ValueError, TypeError):
                    logger.warning(f"Could not convert '{value}' to float for column with type {db_type}")
                    return None
            elif db_type == 'boolean':
                return str_value.lower() in ('true', 't', 'yes', 'y', '1')
            elif db_type == 'date':
                try:
                    return datetime.strptime(str_value, "%Y-%m-%d").date()
                except (ValueError, TypeError):
                     logger.warning(f"Could not convert '{value}' to date for column with type {db_type}")
                     return None
            elif 'timestamp' in db_type:
                try:
                    return datetime.fromisoformat(str_value.replace("Z", "+00:00"))
                except (ValueError, TypeError):
                    logger.warning(f"Could not convert '{value}' to timestamp for column with type {db_type}")
                    return None
            # For text, varchar, etc., just return the stripped string
            elif db_type in ('text', 'character varying'):
                return str_value
        
        # Fallback for when schema is not available (old behavior, but safer)
        # We avoid converting to int unless we are sure it's not an ID.
        # This is not perfect, but safer than aggressive int conversion.
        if str_value.lower() in ('true', 't', 'yes', 'y', '1'):
            return True
        if str_value.lower() in ('false', 'f', 'no', 'n', '0'):
            return False
        
        # Try float
        if '.' in str_value:
            try:
                return float(str_value)
            except (ValueError, TypeError):
                pass

        return str_value
